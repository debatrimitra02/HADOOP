import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Scanner;
import java.util.StringTokenizer;

public class CorpusCaculator 
{	
	//static InputStream filepath;
	public static void main(String[] args) 
	{
		///// ====== Initialize and getting the outputs generated by map-reduce jobs ===== ////			
		String input = "D:/Hadoop/Corpus.txt";
		String sentence_word_counts = "D:/Hadoop/sentence_count.txt";  // needed for computing probabilities
		// sentence_count generated by SentenceWordCount.java in map-reduce
		HashMap<String, Float> Sentences = new HashMap<String, Float>(); // final probabilities will be here
		System.out.println("Reading and initializing probability table of sentences"); 		
		compute_prob_initialize(Sentences);   ///initialize all the entries with probability = 1 

		HashMap<Integer, Integer> Sentence_Words = new HashMap<Integer, Integer>(); 
		//hashmap to hold the o/p of map-reduce job to count no of sentences with 1,2,..N words...  
		int N=load_map_reduce_op_int( Sentence_Words,sentence_word_counts);
		//so, N is the max number of words in a sentence in the corpus - so, max position of a word is N in a sentence		
		//so, I will read the N job outputs generated by map-reduce which is the counts of words in pos 1, pos 2,...pos N
		// we need to read each sentence one word in every pass and open the corresponding word count file
		// and update the sentence table by multiplying the Probability  
		
		// We call the compute_prob_pos method for each k=1,2,....N 
		// input should be the sentence count map, initialize prob table 
		
		// all map-reduce jobs in word_position_count should output in outk.txt k=1,2,3,...N
		
		for (int k=1; k<N ;k++)
			compute_prob_position(Sentences,Sentence_Words,k);

		// check if hashing of sentences worked
		printProbability(Sentences,input);  /// ====> testing
		// This should give the probabilities of each sentence ---- for testing
		
		//// Get top 3 Sentences from the map Sentences 
		/// so now sentences has all the sentences and the probabilities
		
		// iterate over the map and always keep top 3 -- O(n) better than sorting
		
		
		
		
		//File file = new File("D:/Hadoop/Corpus.txt");			
	}
	
	public static void topKsentences(HashMap<String, Float> Sentences,int K)
	{
		String[]Top3_Sentences= new String[K];
		float[]top3_prob=new float[K];
				
		Iterator<String> keySetIterator = Sentences.keySet().iterator();
		while(keySetIterator.hasNext())
		{			
		  String key = keySetIterator.next();
		  if(Sentences.get(key)>top3_prob[0])
		  {
			  top3_prob[2]=top3_prob[1];
			  top3_prob[1]=top3_prob[0]; 
			  top3_prob[0]=Sentences.get(key);
			  
			  Top3_Sentences[2]=Top3_Sentences[1];
			  Top3_Sentences[1]=Top3_Sentences[0];
			  Top3_Sentences[0]=key;
			  
		  }
		  else if(Sentences.get(key)>top3_prob[1])
		  {
			  top3_prob[2]=top3_prob[1];
			  top3_prob[1]=Sentences.get(key);
			  
			  Top3_Sentences[2]=Top3_Sentences[1];
			  Top3_Sentences[1]=key;
		  }
		  else if(Sentences.get(key)>top3_prob[2])
		  {
			  
			  Top3_Sentences[2]=key;
		  }
/*		  for i=1 to n
			       if(a[i]>m1)
			       {    m3=m2; m2=m1; m1=a1;}
			       else if(a[i]>m2)
			       {    m3=m2; m2=a1; }
			       else if (a[i]>m3)
			             m3=a[i];
			             */
		 	  
		}
		
		System.out.println(Top3_Sentences[0]+"\n");
		System.out.println(Top3_Sentences[1]+"\n");
		System.out.println(Top3_Sentences[2]+"\n");
	}
	
	public static void compute_prob_position(HashMap<String, Float> Sentences,
			HashMap<Integer, Integer> Sentence_Words,int k)
	{
		//filepath = new URL("https://www.dropbox.com/s/ubrivo8sh6x1z1h/Corpus.txt?dl=0").openStream();
		//System.out.println("Read Stream");

//		String Filename = ""
		String pathname ="D:/hadoop";
		String Filename = pathname + "Out"+ Integer.toString(k)+".txt";		// pointing to the k-th position count job output
		HashMap<String, Integer> kth_word = new HashMap<String, Integer>(); // map to load the Out1.txt 
		File file = new File(Filename);
		//		String line = null;

		if(file.canRead())
		{
			System.out.println("Can Read");
			// read file here
			try 
			{				
				Scanner sc=new Scanner(file);
				while(sc.hasNextLine())
				{
					//System.out.println(sc.nextLine());  // checked -- reads line by line
					// create the sentences hashmap which will hold it's probability
					//Sentences.put(sc.nextLine(), 1);  	
					String line =sc.nextLine();
					String[] words = line.split(" "); 					
					kth_word.put(words[0], Integer.parseInt(words[1])); 
				}
				sc.close();				
				//Scanner sc2=new Scanner(file);
			} 
			catch (FileNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

		}
		//calculate cummulative sum of sentence count i.e. Num of sentences >= k i.e. sum all the counts with key >= K as key is number of words in a sentence
		Iterator<Integer> keySetIterator = Sentence_Words.keySet().iterator();
		
		int Sum=0;
		int i=0;
		while(keySetIterator.hasNext())
		{			
		  Integer key = keySetIterator.next();
		  if(i>=k)
			  Sum = Sum + Sentence_Words.get(key);
		  i++;
		}
		
		//so we have the words at position k counts i.e.  Count W_i(k) in the map kth_word and the cumulative sentence counts=> so we have the probability;
		// Now we run through each sentence's k th word if exist and multiply the probability at the index
		
		Iterator<String> keySetIterator2 = Sentences.keySet().iterator();
		while(keySetIterator.hasNext())
		{			
		  String key = keySetIterator2.next();   /// so key is the sentence 
		  String[] words = key.split(" "); // words contain all the words of that sentence
		  if(words[k-1]!=null)
		  {
			  //look for the word count in the word_count table
			  int word_count = kth_word.get(words[k-1]);	// word count found - divide by cumulative sum to get the probability
			  float probability = word_count/Sum;
			  float probab=probability*(Sentences.get(key));
			  Sentences.put(key, probab);
		  }
		  
		}
	}
	
	public static int load_map_reduce_op_int(HashMap<Integer, Integer> Sentences,String Filename)
	{
		//filepath = new URL("https://www.dropbox.com/s/ubrivo8sh6x1z1h/Corpus.txt?dl=0").openStream();
		//System.out.println("Read Stream");
		File file = new File(Filename);
		//		String line = null;

		int maximumLength=0;
		if(file.canRead())
		{
			System.out.println("Can Read the file ");
			// read file here
			try 
			{				
				Scanner sc=new Scanner(file);
				while(sc.hasNextLine())
				{
					//System.out.println(sc.nextLine());  // checked -- reads line by line
					// create the sentences hashmap which will hold it's probability					
					String line =sc.nextLine();
					//StringTokenizer tokenizer = new StringTokenizer(line);
					String[] words = line.split(" "); 					
					Sentences.put(Integer.parseInt(words[0]), Integer.parseInt(words[1])); 	
					// reading the size of sentence in first element and Count as second element 
					//get the max possible position
					if(Integer.parseInt(words[0])>maximumLength)
						maximumLength=Integer.parseInt(words[0]);	
				}
				sc.close();				
				//Scanner sc2=new Scanner(file);
			} 
			catch (FileNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}						
		}
		return maximumLength;
	}

	public static void printProbability (HashMap<String, Float> Sentences, String filename)
	{
		File file = new File(filename);
		Scanner sc;
		try 
		{
			sc = new Scanner(file);
			while(sc.hasNextLine())
			{
				System.out.println(sc.nextLine());  // checked -- reads line by line
				// create the sentences hashmap which will hold it's probability
				System.out.println(Sentences.get(sc.nextLine())); // building initial map with all prob =1  	
			}
			sc.close();
		} catch (FileNotFoundException e) 
		{
			// TODO Auto-generated catch block
			e.printStackTrace();
		}	
	}

	public static void compute_prob_initialize(HashMap<String, Float> Sentences)
	{
		//filepath = new URL("https://www.dropbox.com/s/ubrivo8sh6x1z1h/Corpus.txt?dl=0").openStream();
		//System.out.println("Read Stream");
		File file = new File("D:/Hadoop/Corpus.txt");
		//		String line = null;

		if(file.canRead())
		{
			System.out.println("Can Read");
			// read file here
			try 
			{				
				Scanner sc=new Scanner(file);
				while(sc.hasNextLine())
				{
					//System.out.println(sc.nextLine());  // checked -- reads line by line
					// create the sentences hashmap which will hold it's probability
					Sentences.put(sc.nextLine(), (float) 1); // building initial map with all prob =1  	
				}
				sc.close();				
				//Scanner sc2=new Scanner(file);
			} 
			catch (FileNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}						
		}
	}	
}
